{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea63db15",
   "metadata": {},
   "source": [
    "\n",
    "# **AirQ_Part1_xxx.ipynb** — Assignment 1 Orchestration\n",
    "\n",
    "- This notebook orchestrates Assignment 1.\n",
    "- All SQL must live in external `.sql` files under `ddl/`, `etl/`, and `post/`. You can test them in DBeaver.\n",
    "- The notebook only opens a DB connection, runs external `.sql` files, loads CSVs via Pandas, and produces the final dump.\n",
    "\n",
    "**Final folder layout (per‑group, self‑contained)**\n",
    "\n",
    "```\n",
    "BI_Projects/\n",
    "  DWH1_xxx/\n",
    "    csv/       # 15 OLTP CSV files\n",
    "    date/      # Tables X and Y CSVs (do NOT submit the 15 original CSVs; see submission checklist)\n",
    "    ddl/       # DDL only (staging, X/Y, warehouse, optional reset)\n",
    "    etl/       # SQL-first ETL steps: a1_etl01_...sql, a1_etl02_...sql, ...\n",
    "    post/      # Post-ETL checks: a1_check01_...sql, a1_check02_...sql, ...\n",
    "    prov/      # Your auto-generated provenance JSON-LD file\n",
    "    sqldump/   # Export produced by pg_dump\n",
    "    AirQ_Part1_xxx.ipynb\n",
    "    Airq_ERD_dwh_xxx.png / .pdf\n",
    "    group_xxx.txt\n",
    "    Report_Part1_Group_xxx.pdf\n",
    "```\n",
    "> Replace `xxx` in your file names with your **three‑digit** group number everywhere.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f479a2-7ad4-4321-831c-e9350a3121cc",
   "metadata": {},
   "source": [
    "## Contents\n",
    "1. Configuration & preflight (group, paths)  \n",
    "2. Database connection\n",
    "3. Reset & create stg_xxx  \n",
    "4. Load CSVs into stg_xxx (order-sensitive)  \n",
    "5. Reset & create dwh_xxx (dimensions first, then facts)  \n",
    "6. ETL runner (executes `etl/a1_etl*.sql`)  \n",
    "7. Post-ETL checks (5–7 checks with pass/fail)  \n",
    "8. Create database dump\n",
    "9. Packaging sanity check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04273c9",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Configuration & preflight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07def75d-1bd4-4c44-a341-9e6e71742fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Parameters ===\n",
    "# XXX = \"001\"               # # three digits, e.g. \"007\"\n",
    "# ...\n",
    "# XXX = \"031\"               # # three digits, e.g. \"007\"\n",
    "# ...\n",
    "# XXX = \"071\"               # # three digits, e.g. \"007\"\n",
    "# ...\n",
    "# XXX = \"199\"               # # three digits, e.g. \"007\"\n",
    "XXX = \"006\"               # # three digits, e.g. \"007\"\n",
    "\n",
    "VERBOSE_SQL = False             # print progress when running .sql files\n",
    "LOAD_ORDER_CSV = []             # or fill later in Section 4\n",
    "LOAD_ORDER_DATA = []            # or fill later in Section 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93ae5428-70cc-4237-b3ff-c228686b3271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, time\n",
    "import shutil, subprocess, os\n",
    "import json, hashlib\n",
    "\n",
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "from urllib.parse import quote_plus\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import pandas as pd\n",
    "import sqlparse\n",
    "from sqlalchemy import create_engine, text, engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "004841f2-fb79-4e7f-88f3-9eed76c28bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV dir: /Users/kerimhalilovic/Documents/GitHub/bi-2025W-ass1/csv\n",
      "Data dir: /Users/kerimhalilovic/Documents/GitHub/bi-2025W-ass1/data\n",
      "DDL dir: /Users/kerimhalilovic/Documents/GitHub/bi-2025W-ass1/ddl\n",
      "ETL dir: /Users/kerimhalilovic/Documents/GitHub/bi-2025W-ass1/etl\n",
      "Postchecks dir: /Users/kerimhalilovic/Documents/GitHub/bi-2025W-ass1/post\n",
      "SQLdump dir: /Users/kerimhalilovic/Documents/GitHub/bi-2025W-ass1/sqldump\n"
     ]
    }
   ],
   "source": [
    "# === Toggles & paths ===\n",
    "root_dir = Path.cwd()\n",
    "csv_dir = root_dir / \"csv\"\n",
    "data_dir = root_dir / \"data\"\n",
    "ddl_dir = root_dir / \"ddl\"\n",
    "etl_dir = root_dir / \"etl\"\n",
    "post_dir = root_dir / \"post\"\n",
    "sqldump_dir = root_dir / \"sqldump\"\n",
    "\n",
    "SCHEMA_STG = f\"stg_{XXX}\"\n",
    "SCHEMA_DWH = f\"dwh_{XXX}\"\n",
    "\n",
    "# files we expect in the ddl subfolder\n",
    "STG_RESET  = ddl_dir / f\"airq_reset_stg_{XXX}.sql\"\n",
    "STG_CREATE = ddl_dir / f\"airq_create_stg_{XXX}.sql\"\n",
    "STG_EXT    = ddl_dir / f\"airq_create_ext_{XXX}.sql\"\n",
    "DWH_RESET  = ddl_dir / f\"airq_reset_dwh_{XXX}.sql\"\n",
    "DWH_CREATE = ddl_dir / f\"airq_create_dwh_{XXX}.sql\"\n",
    "\n",
    "print(\"CSV dir:\", csv_dir)\n",
    "print(\"Data dir:\", data_dir)\n",
    "print(\"DDL dir:\", ddl_dir)\n",
    "print(\"ETL dir:\", etl_dir)\n",
    "print(\"Postchecks dir:\", post_dir)\n",
    "print(\"SQLdump dir:\", sqldump_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5448ef2-8227-465c-a00c-6883b8535a3e",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Make database connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8816fcb6-7038-486e-ae99-2efc346eeccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Password for grp_006@localhost:5432/airq (leave empty if not needed):  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting via: postgresql+psycopg2://\\1:***@localhost:5432/airq\n",
      "SET ROLE grp_006 ✓\n",
      "current_user: grp_006\n"
     ]
    }
   ],
   "source": [
    "import getpass as gp\n",
    "\n",
    "# === Minimal config & connect ===\n",
    "DB_USER = f\"grp_{XXX}\"\n",
    "DB_NAME = \"airq\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "\n",
    "# a password is asked once per run; enter empty password if your local pg_hba allows trust/peer\n",
    "pw = gp.getpass(f\"Password for {DB_USER}@{DB_HOST}:{DB_PORT}/{DB_NAME} (leave empty if not needed): \")\n",
    "DSN = f\"postgresql+psycopg2://{DB_USER}:{quote_plus(pw)}@{DB_HOST}:{DB_PORT}/{DB_NAME}\" if pw \\\n",
    "      else f\"postgresql+psycopg2://{DB_USER}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "def _mask_dsn(dsn: str) -> str:\n",
    "    try:\n",
    "        return str(engine.make_url(dsn).set(password=\"***\"))\n",
    "    except Exception:\n",
    "        return re.sub(r\"://([^:@]+)(?::[^@]*)?@\", r\"://\\\\1:***@\", dsn)\n",
    "\n",
    "engine = create_engine(DSN, future=True, pool_pre_ping=True)\n",
    "print(\"Connecting via:\", _mask_dsn(DSN))\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    # best-effort: set the role if it exists; don't crash if not\n",
    "    try:\n",
    "        conn.exec_driver_sql(f\"SET ROLE grp_{XXX}\")\n",
    "        print(f\"SET ROLE grp_{XXX} ✓\")\n",
    "    except Exception as e:\n",
    "        print(f\"(no SET ROLE: {e.__class__.__name__})\")\n",
    "    who = conn.exec_driver_sql(\"select current_user\").scalar_one()\n",
    "    print(\"current_user:\", who)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a704b837-f6a0-4c73-b2a5-e3f069aa6df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sqlscript(\n",
    "    path: str,\n",
    "    *,\n",
    "    engine,\n",
    "    progress: bool = True,      # progress/verbosity- show progress OR keep output quiet\n",
    "    add_search_path: bool = False,\n",
    "    schema_dwh: str | None = None,\n",
    "    schema_stg: str | None = None,\n",
    "    title: str | None = None,      # optional title\n",
    "    strip_psql_meta: bool = True,  # psql meta stripping\n",
    "):\n",
    "    \"\"\"\n",
    "    Execute all statements in a .sql file.\n",
    "    - Returns the LAST result set as a pandas.DataFrame if any statement returns rows; else None.\n",
    "    - Set progress=False to suppress progress/header prints (great for check scripts).\n",
    "    \"\"\"\n",
    "\n",
    "    raw = Path(path).read_text(encoding=\"utf-8\")\n",
    "\n",
    "    # Strip psql meta-commands (e.g., \\i, \\set) if requested\n",
    "    if strip_psql_meta:\n",
    "        raw = \"\\n\".join(\n",
    "            line for line in raw.splitlines()\n",
    "            if not line.lstrip().startswith(\"\\\\\")\n",
    "        )\n",
    "\n",
    "    # Optional search_path prologue\n",
    "    prologue = \"\"\n",
    "    if add_search_path:\n",
    "        schs = [s for s in (schema_dwh, schema_stg) if s]\n",
    "        if schs:\n",
    "            prologue = f\"SET search_path TO {', '.join(schs)};\\n\"\n",
    "\n",
    "    script = prologue + raw\n",
    "    stmts = [s.strip() for s in sqlparse.split(script) if s and s.strip(\" ;\\n\\t\")]\n",
    "\n",
    "    if progress:\n",
    "        hdr = f\"▶ {title}\" if title else \"▶ Running SQL script\"\n",
    "        print(f\"{hdr}: {path} ({len(stmts)} statements)\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    last_df = None\n",
    "    with engine.begin() as conn:\n",
    "        for i, stmt in enumerate(stmts, start=1):\n",
    "            if not stmt:\n",
    "                continue\n",
    "            start = time.time()\n",
    "            try:\n",
    "                if progress:\n",
    "                    preview = \" \".join(stmt.split())[:120]\n",
    "                    print(f\"  {i:>3}: {preview} ...\")\n",
    "\n",
    "                cursor = conn.exec_driver_sql(stmt)\n",
    "\n",
    "                if cursor.returns_rows:\n",
    "                    rows = cursor.fetchall()\n",
    "                    cols = cursor.keys()\n",
    "                    last_df = pd.DataFrame(rows, columns=cols)\n",
    "\n",
    "                if progress:\n",
    "                    print(f\"       OK ({time.time() - start:.3f}s)\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # Raise with a helpful preview even when progress=False\n",
    "                preview = \" \".join(stmt.split())[:160]\n",
    "                raise RuntimeError(\n",
    "                    f\"SQL error in statement #{i}: {preview}\"\n",
    "                ) from e\n",
    "\n",
    "    if progress:\n",
    "        print(f\"✅ Done in {time.time() - t0:.2f}s\")\n",
    "\n",
    "    return last_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e6e6b6-2aa7-43b6-8477-41f4b15694b0",
   "metadata": {},
   "source": [
    "## 3) Reset and Create airq schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d04d190-0479-42fb-babe-9d782e26235d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== STAGING-ONLY RESET: stg_006 ==\n"
     ]
    }
   ],
   "source": [
    "print(f\"== STAGING-ONLY RESET: stg_{XXX} ==\")\n",
    "try:\n",
    "    for p in (STG_RESET, STG_CREATE, STG_EXT):\n",
    "        run_sqlscript(p, engine=engine, progress=VERBOSE_SQL)\n",
    "except Exception as e:\n",
    "    print(f\"!! Reset & create failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fc162a",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Load CSV → `stg_xxx` with Pandas `.to_sql()` (you must choose the correct order)\n",
    "\n",
    "Because `stg_xxx` has foreign keys, **order matters**. Fill `LOAD_ORDER_CSV` with base filenames (no `.csv`).  \n",
    "If you leave the list empty, the cell will just print what CSVs it found without loading anything.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "855c8fcd-8591-40fb-b435-58d118e7e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_folder_to_stg(\n",
    "    folder_name: str,\n",
    "    engine,\n",
    "    SCHEMA_STG: str,\n",
    "    load_order=None,\n",
    "    if_exists: str = \"append\",\n",
    "    chunksize: int = 20000,\n",
    "):\n",
    "    global root_dir  # expected to be defined earlier\n",
    "    src_dir = Path(root_dir) / folder_name\n",
    "    if not src_dir.exists():\n",
    "        raise FileNotFoundError(f\"Folder not found: {src_dir}\")\n",
    "\n",
    "    def load_one(name: str):\n",
    "        path = src_dir / f\"{name}.csv\"\n",
    "        if not path.exists():\n",
    "            print(\"Missing CSV:\", path.name)\n",
    "            return 0\n",
    "        df = pd.read_csv(\n",
    "            path,\n",
    "            na_values=[\"\\\\N\"],\n",
    "            keep_default_na=False,\n",
    "            low_memory=False,\n",
    "        )\n",
    "        # Convert any *...from / ...to / ...at* to DATE\n",
    "        for col in df.columns:\n",
    "            col_l = col.lower()\n",
    "            if col_l.endswith((\"from\", \"to\", \"at\")):\n",
    "                df[col] = pd.to_datetime(df[col], format=\"%Y-%m-%d\", errors=\"coerce\").dt.date\n",
    "        # Write\n",
    "        df.to_sql(\n",
    "            name,\n",
    "            con=engine,\n",
    "            schema=SCHEMA_STG,\n",
    "            if_exists=if_exists,\n",
    "            index=False,\n",
    "            method=\"multi\",\n",
    "            chunksize=chunksize,\n",
    "        )\n",
    "        print(f\"Loaded {len(df):,} rows → {SCHEMA_STG}.{name}\")\n",
    "        return len(df)\n",
    "\n",
    "    if not load_order:\n",
    "        discovered = sorted([p.stem for p in src_dir.glob(\"*.csv\")])\n",
    "        print(\"No order set yet. CSVs found:\", discovered)\n",
    "        return\n",
    "\n",
    "    t0 = time.time()\n",
    "    total = 0\n",
    "    for name in load_order:\n",
    "        total += load_one(name)\n",
    "    print(f\"⏱️ Total load time: {time.time() - t0:.2f} seconds · {total:,} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a109ccca-903b-4980-87ff-5092004f0d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 rows → stg_006.tb_country\n",
      "Loaded 16 rows → stg_006.tb_role\n",
      "Loaded 24 rows → stg_006.tb_servicetype\n",
      "Loaded 8 rows → stg_006.tb_readingmode\n",
      "Loaded 4 rows → stg_006.tb_alert\n",
      "Loaded 30 rows → stg_006.tb_param\n",
      "Loaded 12 rows → stg_006.tb_sensortype\n",
      "Loaded 36 rows → stg_006.tb_city\n",
      "Loaded 484 rows → stg_006.tb_employee\n",
      "Loaded 120 rows → stg_006.tb_paramalert\n",
      "Loaded 115 rows → stg_006.tb_paramsensortype\n",
      "Loaded 627 rows → stg_006.tb_sensordevice\n",
      "Loaded 26,316 rows → stg_006.tb_weather\n",
      "Loaded 985,573 rows → stg_006.tb_readingevent\n",
      "Loaded 22,720 rows → stg_006.tb_serviceevent\n",
      "⏱️ Total load time: 61.50 seconds · 1,036,105 rows\n"
     ]
    }
   ],
   "source": [
    "# First, we load the original 15 CSV files in the correct order\n",
    "\n",
    "LOAD_ORDER_CSV = [\n",
    "    \"tb_country\",        # parent of city\n",
    "    \"tb_role\",           # parent of employee\n",
    "    \"tb_servicetype\",    # parent of serviceevent\n",
    "    \"tb_readingmode\",    # parent of readingevent\n",
    "    \"tb_alert\",          # parent of paramalert\n",
    "    \"tb_param\",          # parent of paramalert & paramsensortype & readingevent\n",
    "    \"tb_sensortype\",     # parent of paramsensortype & sensordevice\n",
    "    \"tb_city\",           # child of country; parent of weather & sensordevice\n",
    "    \"tb_employee\",       # child of role; parent of serviceevent\n",
    "    \"tb_paramalert\",     # child of param + alert\n",
    "    \"tb_paramsensortype\",# child of param + sensortype\n",
    "    \"tb_sensordevice\",   # child of sensortype + city; parent of readingevent/serviceevent\n",
    "    \"tb_weather\",        # child of city\n",
    "    \"tb_readingevent\",   # child of sensordevice + param + readingmode\n",
    "    \"tb_serviceevent\",   # child of servicetype + employee + sensordevice\n",
    "]\n",
    "\n",
    "load_folder_to_stg(\"csv\", engine, SCHEMA_STG, load_order=LOAD_ORDER_CSV,  if_exists=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396d963e-3517-425a-a031-c186f30c659b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7 rows → stg_006.tb_environmental_campaign\n",
      "Loaded 17 rows → stg_006.tb_campaign_city\n",
      "⏱️ Total load time: 0.01 seconds · 24 rows\n"
     ]
    }
   ],
   "source": [
    "# Next, we load our \"extra\" tables X and Y\n",
    "\n",
    "LOAD_ORDER_DATA = [\n",
    "    \"tb_environmental_campaign\",\n",
    "    \"tb_campaign_city\",\n",
    "]\n",
    "\n",
    "load_folder_to_stg(\"data\", engine, SCHEMA_STG, load_order=LOAD_ORDER_DATA, if_exists=\"append\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8bd301",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Reset and create **warehouse** (`dwh_xxx`) from DDL file\n",
    "\n",
    "Run `ddl/airq_reset_dwh_xxx.sql` and `ddl/airq_create_dwh_xxx.sql` (dimensions first, then facts; include `etl_load_timestamp`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56719c08-77d7-45c0-8a04-5b1fe82e988a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== DWH-ONLY RESET: dwh_006 ==\n"
     ]
    }
   ],
   "source": [
    "print(f\"== DWH-ONLY RESET: dwh_{XXX} ==\")\n",
    "try:\n",
    "    for p in (DWH_RESET, DWH_CREATE):\n",
    "        run_sqlscript(p, engine=engine, progress=VERBOSE_SQL)\n",
    "except Exception as e:\n",
    "    print(f\"!! Reset & create failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1af517",
   "metadata": {},
   "source": [
    "\n",
    "## 6) SQL-first ETL — run all files in etl/\n",
    "\n",
    "We execute **all** files matching `etl/a1_etl*.sql` in lexicographic order. Every ETL file must begin with `SET search_path TO dwh_xxx, stg_xxx;`  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "518eb416",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = sorted(etl_dir.glob(\"a1_etl*.sql\"))\n",
    "if not steps:\n",
    "    print(\"No ETL step files found in etl/ (expected a1_etl*.sql).\")\n",
    "else:\n",
    "    for s in steps:\n",
    "        run_sqlscript(s, engine=engine, progress=VERBOSE_SQL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117b09c4",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Post‑ETL checks — run files from `post/`\n",
    "\n",
    "Each check should be a small SQL query with `SET search_path TO dwh_xxx, stg_xxx;` and a clear, short interpretation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6b6fe2-dca2-4fd4-8612-97ed3fd90e89",
   "metadata": {},
   "source": [
    "7.1. Dimension **dim_servicetype** contains the same number of rows as table **tb_servicetype** -> OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd8d9620-c799-4f4b-b41b-83c92109d7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dwh_count</th>\n",
       "      <th>stg_count</th>\n",
       "      <th>status_check</th>\n",
       "      <th>run_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>OK</td>\n",
       "      <td>2025-10-23 17:59:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dwh_count  stg_count status_check            run_time\n",
       "0         24         24           OK 2025-10-23 17:59:16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check 1 — example\n",
    "df = run_sqlscript(\"post/a1_check01_example.sql\", engine=engine, progress=VERBOSE_SQL)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dbf506-0ef1-4542-a103-f1ddddd8e2a1",
   "metadata": {},
   "source": [
    "7.2. There are no mismatching names for **typename** between dimension **dim_servicetype** and staging table **tb_servicetype** -> OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a13fefb4-af50-4630-916b-3743ba1f0f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_mismatches_or_missing</th>\n",
       "      <th>status_check</th>\n",
       "      <th>run_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>OK</td>\n",
       "      <td>2025-10-23 17:59:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name_mismatches_or_missing status_check            run_time\n",
       "0                           0           OK 2025-10-23 17:59:16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check 2 — example\n",
    "df = run_sqlscript(\"post/a1_check02_example.sql\", engine=engine, progress=VERBOSE_SQL)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34595cc9",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Create `sqldump/sqldump_airq_dwh_xxx.sql`\n",
    "\n",
    "We run `pg_dump -n dwh_xxx --no-owner --no-privileges` to keep dumps portable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "060bec06-8be2-423c-8e88-7699563fc020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "which pg_dump now: /opt/homebrew/opt/libpq/bin/pg_dump\n",
      "Running: /opt/homebrew/opt/libpq/bin/pg_dump -h localhost -p 5432 -U <user> -d airq -n dwh_006 --no-owner --no-privileges -f /Users/kerimhalilovic/Documents/GitHub/bi-2025W-ass1/sqldump/sqldump_airq_dwh_006.sql\n",
      "✓ Dump created at /Users/kerimhalilovic/Documents/GitHub/bi-2025W-ass1/sqldump/sqldump_airq_dwh_006.sql\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "os.environ[\"PATH\"] = \"/opt/homebrew/opt/libpq/bin:\" + os.environ.get(\"PATH\",\"\")\n",
    "print(\"which pg_dump now:\", shutil.which(\"pg_dump\"))\n",
    "\n",
    "# === Create sqldump/sqldump_airq_dwh_xx.sql (pg_dump) ===\n",
    "sqldump_dir.mkdir(exist_ok=True)\n",
    "outfile = sqldump_dir / f\"sqldump_airq_dwh_{XXX}.sql\"\n",
    "\n",
    "pg_dump = shutil.which(\"pg_dump\") or \"pg_dump\"\n",
    "cmd = [\n",
    "    pg_dump,\n",
    "    \"-h\", DB_HOST,\n",
    "    \"-p\", str(DB_PORT),\n",
    "    \"-U\", DB_USER,\n",
    "    \"-d\", DB_NAME,\n",
    "    \"-n\", f\"dwh_{XXX}\",\n",
    "    \"--no-owner\",\n",
    "    \"--no-privileges\",\n",
    "    \"-f\", str(outfile),\n",
    "]\n",
    "\n",
    "# Avoid echoing the password; supply it via env if provided\n",
    "env = dict(os.environ)\n",
    "if 'pw' in globals() and pw:\n",
    "    env[\"PGPASSWORD\"] = pw\n",
    "\n",
    "print(\"Running:\", \" \".join(cmd).replace(DB_USER, \"<user>\"))\n",
    "try:\n",
    "    subprocess.run(cmd, check=True, env=env)\n",
    "    print(\"✓ Dump created at\", outfile)\n",
    "except Exception as e:\n",
    "    print(\"pg_dump failed; try this manually in a terminal:\\n\", \" \".join(cmd), \"\\nError:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df35aab-0b9a-4fa0-94b0-060691c857b0",
   "metadata": {},
   "source": [
    "## 9) PROV-O (JSON-LD) — lightweight provenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04fab9ce-5dba-40ae-b3c3-3fb91d51f4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PROV-O - student inputs (fill these) ===\n",
    "\n",
    "# 1) Names (required)\n",
    "STUDENT_A = \"Halilovic, Kerim\"\n",
    "STUDENT_B = \"Lukic, Nikola\"\n",
    "\n",
    "# 2) Fact table names in dwh_006 (required)\n",
    "# Student A owns Fact 1; Student B owns Fact 2\n",
    "FACT1_NAME = \"ft_reading_daily\"\n",
    "FACT2_NAME = \"ft_service_event\"\n",
    "\n",
    "# 3) One-sentence grain per fact (required)\n",
    "FACT1_GRAIN = \"One row per sensor device, per measured parameter, per day.\"\n",
    "FACT2_GRAIN = \"One row per service event as recorded in OLTP.\"\n",
    "\n",
    "# 4) 2–4 measures per fact (required). Mark ≥1 as fully additive.\n",
    "FACT1_MEASURES = [\n",
    "    {\"name\": \"cnt_readings\", \"agg\": \"SUM\",  \"fully_additive\": True},\n",
    "    {\"name\": \"sum_data_volume_bytes\", \"agg\": \"SUM\",  \"fully_additive\": True},\n",
    "    {\"name\": \"avg_data_quality\", \"agg\": \"AVG\"},\n",
    "    {\"name\": \"avg_value\", \"agg\": \"AVG\"},\n",
    "    {\"name\": \"cnt_exceed_yellow\", \"agg\": \"SUM\", \"fully_additive\": True},\n",
    "]\n",
    "FACT2_MEASURES = [\n",
    "    {\"name\": \"service_cost_eur\", \"agg\": \"SUM\",  \"fully_additive\": True},\n",
    "    {\"name\": \"service_duration_min\", \"agg\": \"SUM\",  \"fully_additive\": True},\n",
    "    {\"name\": \"service_quality_score\", \"agg\": \"AVG\"},\n",
    "    {\"name\": \"underqualified_flag\", \"agg\": \"SUM\", \"fully_additive\": True},\n",
    "]\n",
    "\n",
    "# 5) OLTP sources per fact (DO NOT list time dimensions here).\n",
    "# List the OLTP snapshot tables that feed each fact and the NON-time dimensions you actually used.\n",
    "FACT1_SOURCES_OLTP = {\n",
    "    \"fact\": [\"tb_readingevent\", \"tb_paramalert\", \"tb_alert\"],\n",
    "    \"dimensions\": {\n",
    "        \"dim_device\": [\"tb_sensordevice\", \"tb_city\", \"tb_country\", \"tb_sensortype\"],\n",
    "        \"dim_parameter\": [\"tb_param\"],\n",
    "        \"dim_sensor_type\": [\"tb_sensortype\"],\n",
    "        \"dim_reading_mode\": [\"tb_readingmode\"],\n",
    "        \"dim_campaign\": [\"tb_environmental_campaign\", \"tb_campaign_city\"],\n",
    "    },\n",
    "    \"why\": \"Student A owns Fact 1; these OLTP tables supply its grain, measures and lookups.\"\n",
    "}\n",
    "FACT2_SOURCES_OLTP = {\n",
    "    \"fact\": [\"tb_serviceevent\"],\n",
    "    \"dimensions\": {\n",
    "        \"dim_device\": [\"tb_sensordevice\", \"tb_city\", \"tb_country\", \"tb_sensortype\"],\n",
    "        \"dim_service_type\": [\"tb_servicetype\"],\n",
    "        \"dim_technician_role\": [\"tb_employee\", \"tb_role\"],\n",
    "        \"dim_campaign\": [\"tb_environmental_campaign\", \"tb_campaign_city\"],\n",
    "    },\n",
    "    \"why\": \"Student B owns Fact 2; these inputs determine its grain and lookups.\"\n",
    "}\n",
    "\n",
    "# 6) Time dimensions (generated; no OLTP lineage here).\n",
    "# Provide ONE or TWO names following the required pattern: dim_time<granularity>\n",
    "TIME_DIMS = [\n",
    "    \"dim_timeday\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f56ec606-d4ab-4d5a-a93e-5f0135a25c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Wrote /Users/kerimhalilovic/Documents/GitHub/bi-2025W-ass1/prov/prov_airq_dwh_006.jsonld\n",
      "Preview (first ~24 lines):\n",
      "\n",
      "{\n",
      "  \"@context\": {\n",
      "    \"prov\": \"http://www.w3.org/ns/prov#\",\n",
      "    \"dct\": \"http://purl.org/dc/terms/\",\n",
      "    \"xsd\": \"http://www.w3.org/2001/XMLSchema#\"\n",
      "  },\n",
      "  \"@graph\": [\n",
      "    {\n",
      "      \"@id\": \"file:csv/tb_alert.csv\",\n",
      "      \"@type\": \"prov:Entity\",\n",
      "      \"dct:title\": \"tb_alert.csv\",\n",
      "      \"dct:format\": \"text/csv\",\n",
      "      \"contentHash\": \"71010ed19fe40c7b53cb87b0f7d8e4e32b79f9c472a37030b076e4aa7f3c1682\"\n",
      "    },\n",
      "    {\n",
      "      \"@id\": \"oltp://tb_campaign_city\",\n",
      "      \"@type\": \"prov:Entity\",\n",
      "      \"dct:title\": \"tb_campaign_city\",\n",
      "      \"dct:format\": \"text/csv\"\n",
      "    },\n",
      "    {\n",
      "      \"@id\": \"file:csv/tb_city.csv\",\n",
      "      \"@type\": \"prov:Entity\",\n",
      "      \"dct:title\": \"tb_city.csv\",\n"
     ]
    }
   ],
   "source": [
    "# Outputs: prov/prov_airq_dwh_XXX.jsonld\n",
    "# Expects: root_dir, csv_dir, etl_dir, XXX (group number as a string), and optionally SQLAlchemy `engine`.\n",
    "\n",
    "def _iso_utc_now():\n",
    "    return datetime.now(timezone.utc).replace(microsecond=0).isoformat().replace(\"+00:00\", \"Z\")\n",
    "\n",
    "def _sha256(path: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def _file_iri(p: Path) -> str:\n",
    "    rel = os.path.relpath(p, root_dir).replace(os.sep, \"/\")\n",
    "    return f\"file:{rel}\"\n",
    "\n",
    "def _csv_path_for(name: str):\n",
    "    \"\"\"Resolve an OLTP table/CSV name to a CSV Path in csv_dir (case-insensitive).\"\"\"\n",
    "    n = (name or \"\").strip()\n",
    "    if not n:\n",
    "        return None\n",
    "    base = n.lower().replace(\".csv\", \"\")\n",
    "    candidates = [f\"{base}.csv\", f\"tb_{base}.csv\"]\n",
    "    lowers = {p.name.lower(): p for p in csv_dir.glob(\"*.csv\")}\n",
    "    for cand in candidates:\n",
    "        if cand in lowers:\n",
    "            return lowers[cand]\n",
    "    return None  # not found; record logical IRI instead\n",
    "\n",
    "GROUP = str(globals().get(\"XXX\", \"000\"))\n",
    "now = _iso_utc_now()\n",
    "schema = f\"dwh_{GROUP}\"\n",
    "\n",
    "# --- Validate inputs (minimal) ---\n",
    "# for s in (STUDENT_A, STUDENT_B):\n",
    "#     assert isinstance(s, str) and s.strip() and \"Surname\" not in s, \"Fill in STUDENT_A / STUDENT_B.\"\n",
    "for nm in (FACT1_NAME, FACT2_NAME):\n",
    "    assert isinstance(nm, str) and nm.startswith(\"ft_\"), \"FACT names should start with ft_.\"\n",
    "for g in (FACT1_GRAIN, FACT2_GRAIN):\n",
    "    assert isinstance(g, str) and len(g.split()) >= 4, \"Provide a short grain sentence per fact.\"\n",
    "for arr in (FACT1_MEASURES, FACT2_MEASURES):\n",
    "    assert isinstance(arr, list) and 1 <= len(arr) <= 6, \"List 1–6 measures per fact.\"\n",
    "assert isinstance(TIME_DIMS, list) and 1 <= len(TIME_DIMS) <= 2, \"Define 1 or 2 time dimensions in TIME_DIMS.\"\n",
    "for tname in TIME_DIMS:\n",
    "    assert isinstance(tname, str), \"Each time dimension name must be a string.\"\n",
    "    assert tname.startswith(\"dim_time\"), \"Time dimension names must start with 'dim_time'.\"\n",
    "    gran = tname[len(\"dim_time\"):]  # suffix after the required prefix\n",
    "    assert gran and gran.isalpha(), \"Provide a non-empty alphabetic granularity after 'dim_time' (e.g., 'day','month').\"\n",
    "# If two dims, require different granularities (case-insensitive)\n",
    "if len(TIME_DIMS) == 2:\n",
    "    g1 = TIME_DIMS[0][len(\"dim_time\"):].lower()\n",
    "    g2 = TIME_DIMS[1][len(\"dim_time\"):].lower()\n",
    "    assert g1 != g2, \"If you define two time dimensions, their granularities must differ (e.g., day vs month).\"\n",
    "\n",
    "# === CSV entities from declared OLTP sources (no time dims here) ===\n",
    "declared = set()\n",
    "def _collect_from_mapping(mapping):\n",
    "    if not mapping:\n",
    "        return\n",
    "    for n in mapping.get(\"fact\", []): declared.add(n)\n",
    "    for _, sources in mapping.get(\"dimensions\", {}).items():\n",
    "        for n in sources: declared.add(n)\n",
    "\n",
    "_collect_from_mapping(FACT1_SOURCES_OLTP)\n",
    "_collect_from_mapping(FACT2_SOURCES_OLTP)\n",
    "\n",
    "csv_entities, csv_index = [], {}\n",
    "for name in sorted({n.strip() for n in declared if str(n).strip()}):\n",
    "    p = _csv_path_for(name)\n",
    "    if p and p.exists():\n",
    "        iri = _file_iri(p)\n",
    "        csv_entities.append({\"@id\": iri, \"@type\": \"prov:Entity\",\n",
    "                             \"dct:title\": p.name, \"dct:format\": \"text/csv\",\n",
    "                             \"contentHash\": _sha256(p)})\n",
    "        csv_index[name] = iri\n",
    "    else:\n",
    "        iri = f\"oltp://{name.lower().replace('.csv','')}\"\n",
    "        csv_entities.append({\"@id\": iri, \"@type\": \"prov:Entity\",\n",
    "                             \"dct:title\": name, \"dct:format\": \"text/csv\"})\n",
    "        csv_index[name] = iri\n",
    "\n",
    "# === ETL scripts as prov:Plan entities (hash for reproducibility) ===\n",
    "etl_list  = globals().get(\"ETL_SCRIPTS_RUN\")\n",
    "etl_paths = [Path(s) for s in etl_list] if etl_list else sorted(etl_dir.glob(\"*.sql\"))\n",
    "etl_entities = [{\"@id\": _file_iri(p), \"@type\": [\"prov:Entity\", \"prov:Plan\"],\n",
    "                 \"dct:title\": p.name, \"dct:format\": \"text/sql\",\n",
    "                 \"contentHash\": _sha256(p)} for p in etl_paths]\n",
    "\n",
    "# === DWH outputs discovered from the database (row counts if engine is available) ===\n",
    "output_entities, output_index = [], {}\n",
    "def _add_output_entity(tname: str, cnt: int | None):\n",
    "    iri = f\"db://airq/{schema}/{tname}\"\n",
    "    ent = {\"@id\": iri, \"@type\": \"prov:Entity\", \"dct:title\": tname}\n",
    "    if cnt is not None: ent[\"rowCount\"] = int(cnt)\n",
    "    output_entities.append(ent)\n",
    "    output_index[tname] = iri\n",
    "\n",
    "try:\n",
    "    if \"engine\" in globals():\n",
    "        from sqlalchemy import text\n",
    "        with engine.connect() as conn:\n",
    "            rows = conn.execute(\n",
    "                text(\"\"\"select table_name\n",
    "                          from information_schema.tables\n",
    "                         where table_schema = :s and table_type='BASE TABLE'\n",
    "                         order by table_name\"\"\"), {\"s\": schema}\n",
    "            ).fetchall()\n",
    "            for (tname,) in rows:\n",
    "                cnt = conn.execute(text(f'select count(*) from \"{schema}\".\"{tname}\"')).scalar()\n",
    "                _add_output_entity(tname, cnt)\n",
    "    else:\n",
    "        # offline: facts + declared time dims must appear in provenance\n",
    "        for t in [FACT1_NAME, FACT2_NAME] + TIME_DIMS:\n",
    "            _add_output_entity(t, None)\n",
    "except Exception as e:\n",
    "    print(\"Note: could not enumerate DWH tables / counts:\", e)\n",
    "    for t in [FACT1_NAME, FACT2_NAME] + TIME_DIMS:\n",
    "        if t not in output_index:\n",
    "            _add_output_entity(t, None)\n",
    "\n",
    "# === Activity for this pipeline run ===\n",
    "activity_id = f\"urn:airq:etl:group:{GROUP}:{now}\"\n",
    "activity = {\n",
    "    \"@id\": activity_id,\n",
    "    \"@type\": \"prov:Activity\",\n",
    "    \"dct:description\": f\"Assignment 1 ETL run for {schema}\",\n",
    "    \"prov:startedAtTime\": now,\n",
    "    \"prov:endedAtTime\": now,\n",
    "    \"prov:used\": [e[\"@id\"] for e in (csv_entities + etl_entities)],\n",
    "    \"prov:generated\": [e[\"@id\"] for e in output_entities],\n",
    "}\n",
    "\n",
    "# === Agents and ownership roles (A owns Fact1; B owns Fact2) ===\n",
    "human_agents = [\n",
    "    {\"@id\": \"urn:person:A\", \"@type\": \"prov:Agent\", \"dct:title\": STUDENT_A},\n",
    "    {\"@id\": \"urn:person:B\", \"@type\": \"prov:Agent\", \"dct:title\": STUDENT_B},\n",
    "]\n",
    "software_agents = [\n",
    "    {\"@id\": \"urn:software:jupyter\",  \"@type\": \"prov:SoftwareAgent\", \"dct:title\": \"Jupyter Notebook\"},\n",
    "    {\"@id\": \"urn:software:postgres\", \"@type\": \"prov:SoftwareAgent\", \"dct:title\": \"PostgreSQL\"},\n",
    "    {\"@id\": \"urn:software:os\",       \"@type\": \"prov:SoftwareAgent\", \"dct:title\": os.name},\n",
    "]\n",
    "activity[\"prov:wasAssociatedWith\"] = [a[\"@id\"] for a in (human_agents + software_agents)]\n",
    "activity[\"prov:qualifiedAssociation\"] = [\n",
    "    {\"@type\": \"prov:Association\", \"prov:agent\": \"urn:person:A\", \"prov:hadRole\": f\"owner:{FACT1_NAME}\"},\n",
    "    {\"@type\": \"prov:Association\", \"prov:agent\": \"urn:person:B\", \"prov:hadRole\": f\"owner:{FACT2_NAME}\"},\n",
    "]\n",
    "\n",
    "# === Derivations from declared OLTP sources -> DWH facts/dims (time dims not included) ===\n",
    "derivations = []\n",
    "def _add_derivations(target_table: str, mapping: dict):\n",
    "    tgt_iri = output_index.get(target_table)\n",
    "    if not tgt_iri: return\n",
    "    # fact-level lineage\n",
    "    for src in mapping.get(\"fact\", []):\n",
    "        src_iri = csv_index.get(src)\n",
    "        if src_iri:\n",
    "            derivations.append({\n",
    "                \"@id\": f\"{tgt_iri}#derivation-{Path(str(src)).stem}\",\n",
    "                \"@type\": \"prov:Derivation\",\n",
    "                \"prov:generatedEntity\": tgt_iri,\n",
    "                \"prov:usedEntity\": src_iri,\n",
    "                \"dct:description\": mapping.get(\"why\", \"\")\n",
    "            })\n",
    "    # dimension-level lineage (non-time dims only)\n",
    "    for dim_name, sources in mapping.get(\"dimensions\", {}).items():\n",
    "        dim_iri = output_index.get(dim_name) or tgt_iri\n",
    "        for src in sources:\n",
    "            src_iri = csv_index.get(src)\n",
    "            if src_iri:\n",
    "                derivations.append({\n",
    "                    \"@id\": f\"{dim_iri}#derivation-{dim_name}-{Path(str(src)).stem}\",\n",
    "                    \"@type\": \"prov:Derivation\",\n",
    "                    \"prov:generatedEntity\": dim_iri,\n",
    "                    \"prov:usedEntity\": src_iri,\n",
    "                    \"dct:description\": f\"{dim_name} ← {Path(str(src)).name}\"\n",
    "                })\n",
    "\n",
    "_add_derivations(FACT1_NAME, FACT1_SOURCES_OLTP)\n",
    "_add_derivations(FACT2_NAME, FACT2_SOURCES_OLTP)\n",
    "\n",
    "# === Minimal metadata on fact entities: grain + measures ===\n",
    "for tname, grain, measures in [\n",
    "    (FACT1_NAME, FACT1_GRAIN, FACT1_MEASURES),\n",
    "    (FACT2_NAME, FACT2_GRAIN, FACT2_MEASURES),\n",
    "]:\n",
    "    iri = output_index.get(tname)\n",
    "    if not iri: continue\n",
    "    for ent in output_entities:\n",
    "        if ent[\"@id\"] == iri:\n",
    "            ent[\"dct:description\"] = f\"Grain: {grain}\"\n",
    "            ent[\"measures\"] = measures\n",
    "            break\n",
    "\n",
    "# === Assemble JSON-LD and write ===\n",
    "doc = {\n",
    "    \"@context\": {\n",
    "        \"prov\": \"http://www.w3.org/ns/prov#\",\n",
    "        \"dct\":  \"http://purl.org/dc/terms/\",\n",
    "        \"xsd\":  \"http://www.w3.org/2001/XMLSchema#\"\n",
    "    },\n",
    "    \"@graph\": csv_entities + etl_entities + output_entities + [activity]\n",
    "              + human_agents + software_agents + derivations,\n",
    "}\n",
    "\n",
    "prov_dir = (root_dir / \"prov\")\n",
    "prov_dir.mkdir(exist_ok=True)\n",
    "outfile = prov_dir / f\"prov_airq_dwh_{GROUP}.jsonld\"\n",
    "with outfile.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(doc, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✓ Wrote\", outfile)\n",
    "print(\"Preview (first ~24 lines):\\n\")\n",
    "for i, line in enumerate(json.dumps(doc, indent=2).splitlines()):\n",
    "    if i >= 24: break\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8faae96",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Submission checklist (put these in your **ZIP**)\n",
    "\n",
    "- `csv/` — 15 \"original\" OLTP snapshot CSVs\n",
    "- `data/` — Your **Table X** and **Table Y** CSVs\n",
    "- `ddl/` — Your DDL scripts \n",
    "- `etl/` — Your `a1_etl*.sql` files (ETL scripts)\n",
    "- `post/` — Your `a1_check*.sql` files (5–7 concise checks)\n",
    "- `prov/` — Your auto-generated provenance JSON-LD file\n",
    "- `sqldump/` — `sqldump_airq_dwh_xxx.sql`  \n",
    "- `AirQ_Part1_xxx.ipynb`\n",
    "- `AirQ_ERD_dwh_xxx.png|pdf`\n",
    "- `group_xxx.txt`\n",
    "- `Report_Part1_Group_xxx.pdf`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef4edfd-4f66-4c98-a8cb-42c10a59290e",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dwh)",
   "language": "python",
   "name": "dwh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
